{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Execution Notebook\n",
        "\n",
        "This notebook provides a Python script to interact with large language models (LLMs) locally using the [Ollama](https://ollama.com/) platform.\n",
        "\n",
        "**Functionality:**\n",
        "1.  **Verifies Ollama Server:** Checks if the Ollama server is running locally.\n",
        "2.  **Interactive Prompting:** Allows you to enter prompts in a loop.\n",
        "3.  **Streaming Responses:** Sends prompts to a specified Ollama model and displays the response in real-time.\n",
        "4.  **Model Selection:** Uses a configurable model (default: `gemma3:4b-it-qat`).\n",
        "5.  **Error Handling:** Basic error handling for server connection and model availability.\n",
        "\n",
        "**Instructions:**\n",
        "1. Follow the README instructions:\n",
        "* Install ollama\n",
        "* Run `ollama serve` command in your terminal.\n",
        "* Pull the desired model using `ollama pull <model_name>` (e.g., `ollama pull gemma3:4b-it-qat`).\n",
        "2. Set the `MODEL_NAME` variable in the second code cell below.\n",
        "3. Run the cells sequentially. The last cell starts an interactive prompt loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ollama\n",
        "import sys\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose the model you previously downloaded (e.g., 'llama3', 'mistral', 'phi3')\n",
        "MODEL_NAME = 'gemma3:4b-it-qat'\n",
        "\n",
        "# Optional: Define an initial prompt or leave it empty to prompt the user.\n",
        "INITIAL_PROMPT = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_ollama_server():\n",
        "    \"\"\"Attempts to connect to the Ollama server to make sure it's running.\"\"\"\n",
        "    try:\n",
        "        # A simple call to list local models. If it fails, the server is down.\n",
        "        ollama.list()\n",
        "        print(\"âœ… Ollama server detected.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error connecting to Ollama server.\")\n",
        "        print(\"Make sure the Ollama application (or service) is running.\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_prompt_with_streaming(model, prompt):\n",
        "    \"\"\"Executes a prompt and displays the response in real-time (streaming).\"\"\"\n",
        "    print(f\"\\nðŸ¤– Sending prompt to '{model}':\")\n",
        "    print(f\">>> {prompt}\")\n",
        "    print(\"\\nResponse:\")\n",
        "\n",
        "    try:\n",
        "        # Use ollama.chat for a chat-like interaction\n",
        "        # stream=True enables word-by-word response\n",
        "        stream = ollama.chat(\n",
        "            model=model,\n",
        "            messages=[{'role': 'user', 'content': prompt}],\n",
        "            stream=True,\n",
        "        )\n",
        "\n",
        "        complete_response = \"\"\n",
        "        for chunk in stream:\n",
        "            response_part = chunk['message']['content']\n",
        "            print(response_part, end='', flush=True) # Display the part without a newline\n",
        "            complete_response += response_part\n",
        "\n",
        "        print(\"\\n\\n--- End of response ---\")\n",
        "        return complete_response # Return the complete response if needed\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Error during generation:\")\n",
        "        # Check if the error is because the model does not exist locally\n",
        "        if \"model\" in str(e) and \"not found\" in str(e):\n",
        "             print(f\"   The model '{model}' does not seem to be downloaded.\")\n",
        "             print(f\"   Try running in the terminal: ollama pull {model}\")\n",
        "        else:\n",
        "            print(f\"   An unexpected error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a_BgIWq52tm",
        "outputId": "0f252ca7-8456-459b-d2eb-903ff768229b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ollama server detected.\n",
            "\n",
            "ðŸ¤– Sending prompt to 'gemma3:4b-it-qat':\n",
            ">>> Hello\n",
            "\n",
            "Response:\n",
            "Hello there! How can I help you today? Do you want to:\n",
            "\n",
            "*   Ask me a question?\n",
            "*   Start a conversation?\n",
            "*   Have me write something (like a story, poem, or code)?\n",
            "*   Just say hi? ðŸ˜Š\n",
            "\n",
            "--- End of response ---\n",
            "(Generation time: 10.86 seconds)\n",
            "\n",
            "ðŸ¤– Sending prompt to 'gemma3:4b-it-qat':\n",
            ">>> How are you?\n",
            "\n",
            "Response:\n",
            "I'm doing well, thank you for asking! As a large language model, I don't really *feel* in the way humans do, but my systems are running smoothly and I'm ready to assist you. ðŸ˜Š \n",
            "\n",
            "How are *you* doing today? Is there anything youâ€™d like to chat about or any questions you have for me?\n",
            "\n",
            "--- End of response ---\n",
            "(Generation time: 10.88 seconds)\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "if not verify_ollama_server():\n",
        "    sys.exit(1) # Exit if the server is not available\n",
        "\n",
        "# Interactive loop to send prompts\n",
        "while True:\n",
        "    if INITIAL_PROMPT:\n",
        "        user_prompt = INITIAL_PROMPT\n",
        "        INITIAL_PROMPT = \"\" # Use only the first time\n",
        "    else:\n",
        "        try:\n",
        "            user_prompt = input(\"\\nEnter your prompt (or 'exit' to quit): \\n>>> \")\n",
        "        except EOFError: # Handle Ctrl+D\n",
        "                print(\"\\nExiting...\")\n",
        "                break\n",
        "\n",
        "    if user_prompt.lower() == 'exit':\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    if not user_prompt:\n",
        "        continue # If nothing is entered, ask again\n",
        "\n",
        "    start_time = time.time()\n",
        "    execute_prompt_with_streaming(MODEL_NAME, user_prompt)\n",
        "    end_time = time.time()\n",
        "    print(f\"(Generation time: {end_time - start_time:.2f} seconds)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
